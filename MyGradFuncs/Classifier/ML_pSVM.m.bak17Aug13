classdef ML_pSVM
    % Permutation SVM for multiple instance learning problem
    % By: Minh Hoai Nguyen (minhhoai@robots.ox.ac.uk)
    % Created: 15-Aug-2013
    % Last modified: 15-Aug-2013

    methods (Static)
        
        
        % Train permutation SVM
        % Bs: 1*n cell structure for bags of instances. Bs{i} is a d*m_i matrix for m_i instances
        % lb: n*1 label vector, lb(i) is 1 or -1.
        % C: C for SVM
        % This solves the optimization problem
        %   min_{w,s,b} 0.5*w'*w + 0.5*s'*s + C*sum_i xi_i
        %          s.t. max_P lb(i)*(w'*Bs{i}*P*s + b) >= 1 - xi_i
        %               xi_i >= 0
        %               s(1) >= ... >= s(m)
        function [w, b, s] = train(Bs, lb, C, options)
            nIter1 = 10;
            nIter2 = 10;
            m = 100; % sample m instances per bag             
            tol = 1e-4;
            
            n = length(Bs);
            d = size(Bs{1},1);
            
            % reorder the bags, putting positive first
            posIdxs = lb == 1;
            nPos = sum(posIdxs);
            nNeg = n - nPos;
            Bs = [Bs(posIdxs), Bs(~posIdxs)]; 
            lb = [ones(nPos,1); -ones(nNeg,1)]; 
            s = 1/m*ones(m,1);
            

            % make each bag to have m instances, by sampling with replacement if necessary
            irIdxs = cell(1, n); % instance replication indexes, instead of replicating the data
                                 % this is also the permutation indexes
            for i=1:n
                m_i = size(Bs{i},2);
                if m_i ~= m
                    irIdxs{i} = randsample(m_i, m, 1);
                else
                    irIdxs{i} = 1:m; 
                end;                
            end;
            
            % initialization             
            BR = zeros(d, n); % bag representative
            for i=1:n
                BR(:,i) = mean(Bs{i}, 2);
            end;                        
            [w, b] = ML_pSVM.linearSvm_primal(BR, lb, {}, C);
                        
            posBR = BR(:,1:nPos);
            negBR = BR(:,1:nNeg); clear BR;
            in2bagMap = 1:nNeg; % mapping from neg data instances to neg bags
                        
            % coordinate descent for permutation of positive bags
            for iter1=1:nIter1                
                % update the order of instances
                IS = zeros(m, n); % instance score
                for i=1:n
                    score_i = Bs{i}(:,irIdxs{i})'*w; % no need to add bias b
                    [IS(:,i), sortedIdxs] = sort(score_i, 'descend');
                    irIdxs{i} = irIdxs{i}(sortedIdxs);                                   
                end;
                
                % update s, by solving a quad prog
                objVal1 = ML_pSVM.cmpObj(Bs, lb, C, w, b, s);
                [s, b, objVal] = ML_pSVM.update_s(IS, lb, C);
                objVal = objVal + 0.5*(w'*w);
                objVal2 = ML_pSVM.cmpObj(Bs, lb, C, w, b, s);
                fprintf('=======> iter: %d, preUpdateS objVal: %g, afterUpdateS objVal: %g, ret ObjVal: %g \n', ...
                    iter1, objVal1, objVal2, objVal);
                
                
                % update bag representative for positive                
                for i=1:nPos                    
                    posBR(:,i) = Bs{i}(:, irIdxs{i})*s;                    
                end;
                
                % constraint generation loop, for permuations of instances in negative bags
                for iter2=1:nIter2
                    % figure out the slack group from the in2bagMap
                    slackGrp = {};
                    grpCnt = 0;
                    for j=1:nNeg
                        idxs = find(in2bagMap == j); % instances from bag j
                        if length(idxs) > 1
                            grpCnt = grpCnt + 1;
                            slackGrp{grpCnt} = idxs + nPos;
                        end;
                    end;
                    
%                     [w, b, ~, dualObj] = ML_pSVM.linearSvm([posBR, negBR], ...
%                         [ones(nPos,1); -ones(size(negBR,2),1)], slackGrp, C);
%                     objVal = dualObj + 0.5*(s'*s);                    
                    
                    [w, b, primalObj] = ML_pSVM.linearSvm_primal([posBR, negBR], ...
                        [ones(nPos,1); -ones(size(negBR,2),1)], slackGrp, C);
                    objVal = primalObj + 0.5*(s'*s);
                    fprintf('  iter: %d-%d, objVal: %g\n', iter1, iter2, objVal);
                    score4neg = negBR'*w + b;
                    
                    % find the max score for negative bags
                    score4negBag = -inf(nNeg,1);      
                    for j=1:nNeg
                        idxs = find(in2bagMap == j); % instances from bag j
                        if ~isempty(idxs)
                            score4negBag(j) = max(score4neg(idxs));                                                    
                        end;
                    end
                    
                    
%                     % remove untight negative constraints
%                     removeIdxs = score4neg < -1 - tol;
%                     negBR(:, removeIdxs) = [];
%                     in2bagMap(removeIdxs) = [];
%                     fprintf('  nNegRemoved: %d\n', sum(removeIdxs));
                    
                    % generate most violated constraints for negative
                    newNegBR = zeros(d, nNeg);
                    for i=nPos+1:n
                        score_i = Bs{i}(:,irIdxs{i})'*w; % no need to add bias b
                        [~, sortedIdxs] = sort(score_i, 'descend');
                        irIdxs{i} = irIdxs{i}(sortedIdxs);                                   
                        
                        newNegBR(:,i-nPos) = Bs{i}(:, irIdxs{i})*s;
                    end;
                    
                    score4newNeg = newNegBR'*w + b;
                    
                    newConstrIdxs = and(score4newNeg > score4negBag + tol, score4newNeg > -1 + tol);
                    
                    nNewConstr = sum(newConstrIdxs);
                    if nNewConstr == 0 
                        fprintf('    No new constraint, termininating CG\n');
                        break;
                    end 
                    
                    newIn2bagMap = 1:nNeg;
                    in2bagMap = [in2bagMap, newIn2bagMap(newConstrIdxs)];
                    negBR = cat(2, negBR, newNegBR(:,newConstrIdxs));
                    fprintf('    nNewConstr: %d, total: %d\n', nNewConstr, size(negBR,2));
                end
            end;
        end;

        
        
        % prediction
        % Bs: 1*n cell structure for bags of instances. Bs{i} is a d*m_i matrix for m_i instances
        % w: weight vector of SVM, b: bias term
        % s: distribution weight vecor of non-increasding order        
        function score = predict(Bs, w, b, s)
            m = length(s);
            n = length(Bs);
            score = zeros(n, 1);
            for i=1:n
                score_i = Bs{i}'*w;
                if size(Bs{i},2) ~= m
                    idxs = randsample(size(Bs{i},2), m, 1);
                else
                    idxs = 1:m;
                end;
                score_i2 = score_i(idxs);
                score_i2 = sort(score_i2, 'descend');
                score(i) = score_i2'*s;
            end;            
            score = score + b;
        end
        
        % compute the objective value
        function [objVal, xi] = cmpObj(Bs, lb, C, w, b, s)
            n = length(Bs);
            m = length(s);
            score = zeros(n, 1);
            for i=1:n
                score_i = Bs{i}'*w; 
                
                if length(score_i) == m
                    idxs = 1:m;                    
                else
                    idxs = randsample(length(score_i), m, 1);
                end
                score_i2 = score_i(idxs);
                score_i2 = sort(score_i2, 'descend');
                score(i) = score_i2'*s;                    
            end;
            score = score + b;
            xi = max(0, 1 - lb.*score);
            objVal = 0.5*(w'*w) + 0.5*(s'*s) + C*sum(xi);
        end
        
        
        % D: d*n matrix
        % lb: n*1 vector of 1 or -1 for label
        % Solve the quadratic:
        %   min_{s, b} 0.5*||s||^2 + C\sum_i xi_i
        %          st. lb(i)*(s'*D(:,i) + b) >= 1 - xi_i
        %              xi_i >= 0
        %              s_1 >= s_2 >= ... >= s_d
        function [s, b, objVal, xi] = update_s(D, lb, C)
            [d, n] = size(D);
            
            % variable x = [s; b; xi] = [s_1; ...; s_d; b; xi_1; ...; xi_n];
            cplex = Cplex('pSVM');
            cplex.Model.sense = 'minimize';
            
            d1  = d+1;  
            d1n = d1+n; % number of variables
            
            % Add linear part of the objective
            obj = zeros(d1n,1);
            obj(d1+1:d1n) = C;
            
            % Add to the objective: C*sum_i xi_i
            % cplex.Model.obj = obj; % this syntax doesn't work, use the below
            cplex.addCols(obj);
            
            
            % Add to objective: 0.5*||s||^2;
            Q = zeros(d1n, d1n); % allocate d non-zero entry for Q
            for j=1:d                
                Q(j,j) = 1;                
            end
            cplex.Model.Q = sparse(Q);
                        
            % Add constraint: xi_i >= 0                                    
            lbound = -inf(d1n,1);
            lbound(d1+1:d1n) = 0;
            cplex.Model.lb = lbound;
            
            % Add constraint that lb(i)*(s'*D(:,i) + b) >= 1 - xi_i
            for i=1:n
                constrVec = zeros(1, d1n);
                constrVec(1:d) = lb(i)*D(:,i);
                constrVec(d1)  = lb(i);
                constrVec(d1 + i) = 1;
                constrVec = sparse(constrVec);
                cplex.addRows(1, constrVec, inf);
            end;

            % Add constraint: s_1 >= s_2 >= ... >= s_d            
            for i=1:d-1
                constrVec = zeros(1, d1n);
                constrVec([i,i+1]) = [1, -1];
                constrVec = sparse(constrVec);
                cplex.addRows(0, constrVec, inf);
            end;
%             % <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<TESTING
%             % Add constraint that half of s should be zero
%             for i=ceil(d/2):d
%                 constrVec = zeros(1, d1n);
%                 constrVec(i) = 1;
%                 cplex.addRows(0, sparse(constrVec), 0);
%             end            
%             % <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<TESTING
            
            % Callback function for display
            cplex.DisplayFunc = []; %@disp;
                        
            % solve
            cplex.solve();
            x = cplex.Solution.x;    
            objVal = cplex.Solution.objval;
            s = x(1:d);
            b = x(d1);
            xi = x(d1+1:end);
        end
        
        
        % see linearSvm for explanation of inputs
        function [w, b, objVal, xi] = linearSvm_primal(D, lb, slackGrp, C)
            [d, n] = size(D);
            
            % variable x = [s; b; xi] = [s_1; ...; s_d; b; xi_1; ...; xi_n];
            cplex = Cplex('pSVM.linearSVM_primal');
            cplex.Model.sense = 'minimize';
            
            d1  = d+1;
            d1n = d1+n; % number of variables
            
            % Add linear part of the objective
            obj = zeros(d1n,1);
            obj(d1+1:d1n) = C;
            
            % Add to the objective: C*sum_i xi_i
            % cplex.Model.obj = obj; % this syntax doesn't work, use the below
            cplex.addCols(obj);
            
            
            % Add to objective: 0.5*||w||^2;
            Q = zeros(d1n, d1n); % allocate d non-zero entry for Q
            for j=1:d                
                Q(j,j) = 1;                
            end
            cplex.Model.Q = sparse(Q);
                        
            % Add constraint: xi_i >= 0                                    
            lbound = -inf(d1n,1);
            lbound(d1+1:d1n) = 0;
            cplex.Model.lb = lbound;
            
            % Add constraint that lb(i)*(w'*D(:,i) + b) >= 1 - xi_i
            for i=1:n
                constrVec = zeros(1, d1n);
                constrVec(1:d) = lb(i)*D(:,i);
                constrVec(d1)  = lb(i);
                constrVec(d1 + i) = 1;
                constrVec = sparse(constrVec);
                cplex.addRows(1, constrVec, inf);
            end;
            
            % Add constraint for slack groups
            for j=1:length(slackGrp)
                slackGrp_j = slackGrp{j};
                for i=2:length(slackGrp_j)
                    constrVec = zeros(1, d1n);
                    constrVec(d1+slackGrp_j(1)) = 1;
                    constrVec(d1+slackGrp_j(i)) = -1;
                    constrVec = sparse(constrVec);
                    cplex.addRows(0, constrVec, 0);
                end                
            end;
            
            % Callback function for display
            cplex.DisplayFunc = []; %@disp;
                        
            % solve
            cplex.solve();
            x = cplex.Solution.x;    
            objVal = cplex.Solution.objval;
            w = x(1:d);
            b = x(d1);
            xi = x(d1+1:end);            
        end
        
        % D:  d*n data matrix for n data points
        % lb: n*1 label vector of 1, -1
        % slackGrp: a 1*m cell structure for m group of data ids.
        %   slackGrp{i} is the ids of data points that share the same slack variable        
        %   Positive data and negative data points can share the same slack variable
        %   For computation reason, it is better not to have group of with a single id        
        %   A data point does not have to be in a group (ie., a single slack for its own).
        %   A data point cannot belong two two groups
        %       e.g., slackGrp = {}, all data points have separate slack
        %       e.g., slackGrp = {[2,3,10]}, points 2, 3, 10 have the same slack
        % C: either a positive scalar or a n*1 positive vector
        %   C is the tradeoff for large margin and less constraint violation
        %   If C is n*1 vector, C(i) is associated with the tradeoff value for i^th data point
        %   Data points have the same slack variables must have the same value of C
        % Let IndSlacks = ids that do not belong to any of slackGrp{j}        
        % This function optimizes
        %   min_{w} 0.5*||w||^2 + sum_{i in IndSlacks} C_i*alpha_i 
        %                       + sum_{slack group j} C_grp_j*beta_j
        %    s.t   y_i*(w'*x_i) + b >= 1 - alpha_i for in in IndSlacks
        %          y_i*(w'*x_i) + b >= 1 - beta_j for all i in slackGrp{j}.
        %          alpha_i >= 0, beta_j >= 0
        % Note: there is no bias term, use linearSvm2 if you need bias term
        function [w, b, alpha, dualObj, primalObj, xi] = linearSvm(D, lb, slackGrp, C)
            tol = 1e-6;
            K = D'*D;
            maxTrial = 30;
            n = length(lb);
            for trialNo=1:maxTrial
                % although K is semidefinite, cplex might complain that it is not because of 
                % finite precision. In this case, we permute D and rerun
                try                                      
                    if trialNo ==1                        
                        [alpha, dualObj] = ML_ShareSlackSvm.kernelSvm_cplex(K, lb, slackGrp, C, [], 0, 1);
                    else
                        perIdxs = randperm(n);                                                
                        revIdxs = zeros(1, n); % for reversed mapping                        
                        revIdxs(perIdxs) = 1:n;
                        
                        % need to permute slackGrp too
                        slackGrp2 = cell(1, length(slackGrp));
                        for i=1:length(slackGrp)
                            slackGrp2{i} = revIdxs(slackGrp{i});
                        end;
               
                        [alpha2, dualObj] = ML_ShareSlackSvm.kernelSvm_cplex(K(perIdxs,perIdxs), lb(perIdxs), slackGrp2, C, [], 0, 1);
                        alpha = alpha2(revIdxs); % reverse the permutation
                    end
                    break; % if no error, break
                catch me                             
                end
            end
            
            w = D*(alpha.*lb);
            
            rawSvmScore = D'*w; % before adding the bias term

            % indexes of positive SVs
            idxs = and(lb == 1, alpha > tol);
            posSvScore = rawSvmScore(idxs); % score of positive support vectors    
            bs(1) = 1 - max(posSvScore);    % If (and I say If) there exists a SV on the margin, 
                                            % it must be the one with max score.
            
            % indexes of negative SVs
            idxs = and(lb == -1, alpha > tol);
            negSvScore = rawSvmScore(idxs); % score of negative SVs
            bs(2) = - min(negSvScore) - 1;  % If (and I say If) there exists a SV on the margin, 
                                            % it must be the one with max score.
                                            % To see this, try: D = [0, 1, -2:0]; lb = [ones(2,1); -ones(3,1)];
                                            % ML_pSVM.linearSvm(D, lb, {}, 0.025);
                                            
            % There must be at least one SV on the margin, because if there is no, we can move the
            % threshold until we have one. However, we don't know if it would be on the positive on
            % negative margin. So we need to test both, and pick the one with lower cost
            
            nonSlgrMems = setdiff(1:length(lb), cat(2, slackGrp{:}));
            cost = zeros(1, 2);
            for j=1:2
                svmScore = rawSvmScore + bs(j);
                xi = max(1 - lb.*svmScore, 0); 
                
                Cxi = C.*xi;                
                slgrCxi = zeros(1, length(slackGrp));
                for i=1:length(slackGrp)
                    slgrCxi(i) = max(Cxi(slackGrp{i}));
                end;
                cost(j) = sum(Cxi(nonSlgrMems)) + sum(slgrCxi);                
            end;
            [minCost, minIdx] = min(cost);
            b = bs(minIdx);
            primalObj = 0.5*(w'*w) + minCost;
            
            fprintf('nTrial: %d, dual: %g, primal: %g\n', trialNo, dualObj, primalObj);

            if dualObj < primalObj - 0.02
                keyboard;
                error('Duality gap is too big, something is wrong');
            end;
        end;
    end    
end

